{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from pymongo import MongoClient\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pymongo\n",
    "import requests\n",
    "from time import sleep\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilote selenium Chrome \n",
    "browser = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to the url\n",
    "browser.get('https://mbasic.facebook.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accept Cookies\n",
    "WebDriverWait(browser, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target username and password\n",
    "username = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='email']\")))\n",
    "password = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='pass']\")))\n",
    "\n",
    "#enter username and password\n",
    "username.clear()\n",
    "username.send_keys(\"*******\")\n",
    "password.clear()\n",
    "password.send_keys(\"*******\")\n",
    "\n",
    "#target the login button and click it\n",
    "login_btn = WebDriverWait(browser, 10).until(\n",
    "    EC.element_to_be_clickable((By.NAME, 'login'))\n",
    ")\n",
    "login_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "buttons = browser.find_elements(By.XPATH, '//*[contains(@class, \"x1i10hfl\")]')\n",
    "for b in buttons:\n",
    "    if \"Plus tard\" in b.text:\n",
    "        b.click()\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Search\n",
    "search = WebDriverWait(browser, 20).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//input[contains(@class, 'x1i10hfl')]\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Enter desired hashtag\n",
    "search.clear()\n",
    "search.send_keys(\"le décès du président Jacques Chirac \")\n",
    "\n",
    "search.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir=\"C:\\\\\"\n",
    "default_dir = os.path.join(list_dir,\"C:/Users/yomna/Downloads/scrapper/Pictures\")\n",
    "opener = urllib.request.build_opener()\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bouton 'Publications' cliqué avec succès.\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "\n",
    "# 1. Trouver et cliquer sur le bouton \"Publications\"\n",
    "try:\n",
    "    # Attendre que le bouton soit cliquable (ajustez le sélecteur si nécessaire)\n",
    "    publications_btn = WebDriverWait(browser, 15).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"mount_0_0_tl\"]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div[1]/div/div[2]/div[1]/div[2]/div/div/div[2]/div/div[2]/div/a'))\n",
    "    )\n",
    "    publications_btn.click()\n",
    "    print(\"Bouton 'Publications' cliqué avec succès.\")\n",
    "    time.sleep(3)  # Attendre le chargement des publications\n",
    "except (NoSuchElementException, TimeoutException) as e:\n",
    "    print(\"Échec : Bouton 'Publications' introuvable ou non cliquable.\", str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats sauvegardés dans C:/Users/yomna/Downloads/scrapper/Pictures\\publications.json\n",
      "Exemple de résultat : [\n",
      "  {\n",
      "    \"text\": \"VDT ( VIE DE TAMOUL)\\n  \\u00b7\\nSuivre\\ne\\no\\no\\nt\\ns\\nd\\nS\\np\\nr\\nn\\n7\\nl\\nu\\n6\\nh\\n6\\nh\\n4\\n2\\n0\\nm\\nb\\n4\\n4\\np\\n2\\n2\\ne\\ng\\n2\\n7\\ng\\n 2\\n1\\ni\\n1\\n f\\n2\\nh\\n5\\ns\\n3\\nm\\ni\\n2\\n0\\nc\\nr\\n0\\na\\n9\\ne\\n0\\n0\\n3\\n0\\nt\\ne\\n  \\u00b7\\nD\\u00e9c\\u00e8s de JACQUES CHIRAC l'ancien Pr\\u00e9sident de la R\\u00e9publique Fran\\u00e7aise \\n \\u00e0 86 ans.\\n\\u00ab L\\u2019Inde, comme la Chine, est \\u00e0 mes yeux une des meilleures illustrations qui soient de la permanence des grandes cultures du monde dans tout ce qu\\u2019elles ont apport\\u00e9 \\u00e0 l\\u2019humanit\\u00e9 d\\u2019intelligence, de connaissance et de spiritualit\\u00e9. \\u00bb\\nToutes les r\\u00e9actions :\\n444\\n444\\n44 commentaires\\n142 partages\\nJ\\u2019aime\\nCommenter\\nPartager\",\n",
      "    \"image\": \"https://static.xx.fbcdn.net/images/emoji.php/v9/t74/1/16/1f1eb_1f1f7.png\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "output_dir = \"C:/Users/yomna/Downloads/scrapper/Pictures\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def scrape_publications():\n",
    "    # Attendre le chargement et faire défiler\n",
    "    for _ in range(3):\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Sélecteur des publications\n",
    "    posts = WebDriverWait(browser, 15).until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, \"xdj266r\") and contains(@class, \"x11i5rnm\")]'))\n",
    "    )\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for post in posts:\n",
    "        try:\n",
    "            # Vérifier si c'est une publication sur Chirac\n",
    "            post_text = post.text.lower()\n",
    "            if \"chirac\" not in post_text and \"décès\" not in post_text:\n",
    "                continue\n",
    "            \n",
    "            # Extraire le texte complet\n",
    "            full_text = post.text\n",
    "            \n",
    "            # Extraire la première image valide\n",
    "            img_url = None\n",
    "            images = post.find_elements(By.TAG_NAME, 'img')\n",
    "            for img in images:\n",
    "                img_src = img.get_attribute('src')\n",
    "                if img_src and img_src.startswith('http'):\n",
    "                    img_url = img_src\n",
    "                    break\n",
    "            \n",
    "            # Ajouter au résultat\n",
    "            result.append({\n",
    "                'text': full_text,\n",
    "                'image': img_url if img_url else None  # None si pas d'image trouvée\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur sur une publication : {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sauvegarde en JSON\n",
    "    output_path = os.path.join(output_dir, 'publications.json')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Résultats sauvegardés dans {output_path}\")\n",
    "    print(f\"Exemple de résultat : {json.dumps(result[:1], indent=2) if result else 'Aucun résultat'}\")\n",
    "\n",
    "# Exécution\n",
    "scrape_publications()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with mongo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 15 documents insérés avec succès\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"  # À adapter selon votre configuration\n",
    "DB_NAME = \"chirac_db\"\n",
    "COLLECTION_NAME = \"publications\"\n",
    "JSON_FILE = \"C:/Users/yomna/Downloads/scrapper/Pictures/publications.json\"\n",
    "\n",
    "def get_mongo_collection():\n",
    "    \"\"\"Établit la connexion à MongoDB\"\"\"\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "    return db[COLLECTION_NAME]\n",
    "\n",
    "def import_json_to_mongodb():\n",
    "    # 1. Vérifier que le fichier existe\n",
    "    if not os.path.exists(JSON_FILE):\n",
    "        print(f\"Erreur: Le fichier {JSON_FILE} n'existe pas\")\n",
    "        return\n",
    "\n",
    "    # 2. Lire le fichier JSON\n",
    "    with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            publications = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erreur de lecture du JSON: {e}\")\n",
    "            return\n",
    "\n",
    "    # 3. Connexion à MongoDB\n",
    "    collection = get_mongo_collection()\n",
    "    \n",
    "    # 4. Nettoyage des données avant insertion\n",
    "    processed_data = []\n",
    "    for pub in publications:\n",
    "        # Normalisation des champs\n",
    "        processed = {\n",
    "            'text': pub.get('text', '').strip(),\n",
    "            'image': pub.get('image'),\n",
    "            'source': 'facebook',  # Ajout d'un champ supplémentaire\n",
    "            'import_date': datetime.datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Filtrage des publications vides\n",
    "        if processed['text']:\n",
    "            processed_data.append(processed)\n",
    "\n",
    "    # 5. Insertion en masse avec gestion des erreurs\n",
    "    if processed_data:\n",
    "        try:\n",
    "            result = collection.insert_many(processed_data, ordered=False)\n",
    "            print(f\"✅ {len(result.inserted_ids)} documents insérés avec succès\")\n",
    "            \n",
    "            # Création d'un index sur le texte pour éviter les doublons\n",
    "            collection.create_index([('text', 'text')])\n",
    "            \n",
    "        except BulkWriteError as e:\n",
    "            print(f\"⚠ Certains documents n'ont pas pu être insérés:\")\n",
    "            for err in e.details['writeErrors']:\n",
    "                print(f\"- Document {err['index']}: {err['errmsg']}\")\n",
    "            \n",
    "            # Compter les documents insérés malgré les erreurs\n",
    "            success_count = len(processed_data) - len(e.details['writeErrors'])\n",
    "            print(f\"{success_count} documents ont tout de même été insérés\")\n",
    "\n",
    "    else:\n",
    "        print(\"Aucune donnée valide à importer\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import datetime  # Pour le champ import_date\n",
    "    import_json_to_mongodb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb_scraper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
